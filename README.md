# DESAFIO BIG DATA - TURMA INDRA UNIESP


## üìå ESCOPO DO DESAFIO
* Neste desafio ser√£o feitas as ingest√µes dos dados que est√£o na pasta dados/dados_entrada onde vamos ter alguns arquivos .csv de um banco de vendas.

       - VENDAS.CSV
       - CLIENTES.CSV
       - ENDERECO.CSV
       - REGIAO.CSV
       - DIVISAO.CSV

* Seu trabalho como engenheiro de dados √© prover dados em uma pasta dados/dados_saida em .csv para ser consumido por um relat√≥rio em PowerBI que dever√° ser constru√≠di dentro da pasta 'app'.

### üìë ETAPAS

* **Etapa 1** - Enviar os arquivos para o HDFS.
* **Etapa 2** - Criar tabelas no Hive.

              - TBL_VENDAS
              - TBL_VENDAS_STG
              - TBL_CLIENTES
              - TBL_CLIENTES_STG
              - TBL_ENDERECO
              - TBL_ENDERECO_STG
              - TBL_REGIAO
              - TBL_REGIAO_STG
              - TBL_DIVISAO
              - TBL_DIVISAO_STG
* **Etapa 3** - Processar os dados no Spark Efetuando suas devidas transforma√ß√µes.
* **Etapa 4** - Gravar as informa√ß√µes em tabelas.

              - FT_VENDAS
              - DIM_CLIENTES
              - DIM_TEMPO
              - DIM_LOCALIDADE
* **Etapa 5** - Exportar os dados para a pasta dados/dados_saida
* **Etapa 6** - Criar e editar o PowerBI com os dados que voc√™ trabalhou.
  * No PowerBI criar gr√°ficos de vendas.
* **Etapa 7** - Criar uma documenta√ß√£o com os testes e etapas do projeto.

### REGRAS

* Campos strings vazios dever√£o ser preenchidos com 'N√£o informado'.
* Campos decimais ou inteiros nulos ou vazios, devers√£o ser preenchidos por 0.
* Atentem-se a modelagem de dados da tabela FATO e Dimens√£o.
* Na tabela FATO, pelo menos a m√©trica valor de vnda √© um requisito obrigat√≥rio.
* Nas dimens√µes dever√° conter valores √∫nicos, n√£o dever√° conter valores repetidos.
   
### INSTRU√á√ïES

* Executar a malha do Hive dentro do Container hive-server.
* Executar o processamento do Spark dentro do container spark.


## ‚úîÔ∏è ESTRUTURA E RESOLUTIVA

### :construction: Projeto em constru√ß√£o :construction:

Dentro do Container, aliadas as pastas j√° existentes, foram agregadas mais algumas pastas com scripts para execu√ß√£o do jobs da melhor forma.

Abaixo est√£o listadas as pastas e arquivos, representando a execu√ß√£o da Etapa 1, 2, 3, 4 e 5, com a descri√ß√£o do que representa cada conte√∫do:

* **app**: Pasta que constar√° o arquivo de execu√ß√£o do Power BI.
* **dados**
  * **dados_entrada**: Arquivos .csv que ser√£o inputadas dentro da Tabela Externa Hive no HDFS.
  * **dados_saida**: Arquivos .csv com as tabelas dimens√£o e FATO, ap√≥s tratamento via Spark, que ser√£o utilizadas para a reprodu√ß√£o no Power BI.
* **hqls**: Pasta que conter√° os arquivos .hql (scripts Hive) para execu√ß√£o. Cada tabela ter√° uma pasta diferente para fins de organiza√ß√£o. Cada pasta das tabelas ter√£o tr√™s arquivos, sendo: **create-external-table-stg** (respons√°vel por criar a planilha externa do HDFS com seus respectivos metadados); **create-managed-table-wrk** (respons√°vel por criar as tabelas internas tamb√©m dentro do Hive, mas que poder√° ser manipulada para seus devidos tratamentos); **insert-table-wrk** (script que levar√° os dados da tabela externa para a tabela interna, com o acr√©scimo da dt_foto, ou seja, a data da foto/informa√ß√µes).
  * **hql-clientes**
  * **hql-divisao**
  * **hql-endereco**
  * **hql-regiao**
  * **hql-vendas**
* **malha**: Pasta onde constar√° os scripts para ingest√£o das tabelas e execu√ß√£o do c√≥digo em Spark
  * *jobs_hive*: Script para executar outros scripts relativos √† cria√ß√£o da tabela externa e interna, com os seus devidos testes para confirma√ß√£o se os dados realmente foram inseridos nos conformes.
  * *jobs_spark*: Script para executar o c√≥digo criado em pyspark para tratamento dos dados da tabela interna e cria√ß√£o das dimens√µes e tabela FATO.
* **scripts**: Pasta que conter√° todos os scripts em bash, para **cria√ß√£o de tabelas**, **inser√ß√£o dos dados na tabela externa e interna**, nessa pasta tamb√©m estar√° o arquivo .py que realizar√° o tratamento em Spark da tabela interna.
  * **create_tables**: Pasta que conter√° os scripts para cria√ß√£o das 10 tabelas dentro do Hive (internas e externas), repassando as vari√°veis via Shell Script.
  * *insert_data_worked_table*: Script que passa vari√°veis e executa o .hql de inser√ß√£o de dados na tabela interna derivado da tabela externa.
  * *update_data_external_table*: Script que passa var√≠√°veis e informa o path do arquivo .csv para constru√ß√£o da tabela Externa. 
  * *job_processor*: Script python. Ele ser√° respons√°vel pelo tratamento dos dados da tabela interna.
* *rollback*: Script para dele√ß√£o dos arquivos .csv do HDFS e das tabelas e BD do Hive, em caso de erro na execu√ß√£o das tabelas.
* *rollout*: Script para cria√ß√£o das pastas no HDFS e para chamar os scrips que criam as tabelas.

### ‚öôÔ∏è Executando os testes

### üìä Dashboard


### üîß Ferramentas utilizadas
- ``Shell Script``
- ``Hive / HQL``
- ``Spark``
- ``Power BI``

### ‚úíÔ∏è Autor do desafio

* **Caiu√° Fran√ßa** - *desafio_bigdata_final* - [caiuafranca](https://github.com/caiuafranca/desafio_bigdata_final)



